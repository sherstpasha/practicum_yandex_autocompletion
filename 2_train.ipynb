{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7992772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>input_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my mom amp i love u we had a very sad day june...</td>\n",
       "      <td>[1820, 1995, 20766, 1312, 1842, 334, 356, 550,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i'm going to prom with my lovely bride</td>\n",
       "      <td>[72, 1101, 1016, 284, 1552, 351, 616, 14081, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dude that really sucks. just this morning i wa...</td>\n",
       "      <td>[67, 2507, 326, 1107, 22523, 13, 655, 428, 332...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thanks for following me back xoxoxo</td>\n",
       "      <td>[27547, 329, 1708, 502, 736, 2124, 1140, 1140,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>quot hi quot problem solved.</td>\n",
       "      <td>[421, 313, 23105, 23611, 1917, 16019, 13, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152690</th>\n",
       "      <td>say hi to my friends kirsty, maree and anh i m...</td>\n",
       "      <td>[16706, 23105, 284, 616, 2460, 479, 667, 88, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152691</th>\n",
       "      <td>quot part of the list quot and quot kick him o...</td>\n",
       "      <td>[421, 313, 636, 286, 262, 1351, 23611, 290, 23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152692</th>\n",
       "      <td>if i hind one of my hoochies thn imma slide th...</td>\n",
       "      <td>[361, 1312, 16222, 530, 286, 616, 289, 2238, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152693</th>\n",
       "      <td>no more heat ftw</td>\n",
       "      <td>[3919, 517, 4894, 277, 4246, 50256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152694</th>\n",
       "      <td>don't forget</td>\n",
       "      <td>[9099, 470, 6044, 50256]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>152695 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "0       my mom amp i love u we had a very sad day june...   \n",
       "1                  i'm going to prom with my lovely bride   \n",
       "2       dude that really sucks. just this morning i wa...   \n",
       "3                     thanks for following me back xoxoxo   \n",
       "4                            quot hi quot problem solved.   \n",
       "...                                                   ...   \n",
       "152690  say hi to my friends kirsty, maree and anh i m...   \n",
       "152691  quot part of the list quot and quot kick him o...   \n",
       "152692  if i hind one of my hoochies thn imma slide th...   \n",
       "152693                                   no more heat ftw   \n",
       "152694                                       don't forget   \n",
       "\n",
       "                                                input_ids  \n",
       "0       [1820, 1995, 20766, 1312, 1842, 334, 356, 550,...  \n",
       "1       [72, 1101, 1016, 284, 1552, 351, 616, 14081, 2...  \n",
       "2       [67, 2507, 326, 1107, 22523, 13, 655, 428, 332...  \n",
       "3       [27547, 329, 1708, 502, 736, 2124, 1140, 1140,...  \n",
       "4        [421, 313, 23105, 23611, 1917, 16019, 13, 50256]  \n",
       "...                                                   ...  \n",
       "152690  [16706, 23105, 284, 616, 2460, 479, 667, 88, 1...  \n",
       "152691  [421, 313, 636, 286, 262, 1351, 23611, 290, 23...  \n",
       "152692  [361, 1312, 16222, 530, 286, 616, 289, 2238, 3...  \n",
       "152693                [3919, 517, 4894, 277, 4246, 50256]  \n",
       "152694                           [9099, 470, 6044, 50256]  \n",
       "\n",
       "[152695 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "data_train = pd.read_csv(r\"data\\train.csv\")\n",
    "data_train[\"input_ids\"] = data_train[\"input_ids\"].apply(ast.literal_eval)\n",
    "\n",
    "data_val = pd.read_csv(r\"data\\val.csv\")\n",
    "data_val[\"input_ids\"] = data_val[\"input_ids\"].apply(ast.literal_eval)\n",
    "\n",
    "data_test = pd.read_csv(r\"data\\test.csv\")\n",
    "data_test[\"input_ids\"] = data_test[\"input_ids\"].apply(ast.literal_eval)\n",
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38af0bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pasha\\practicum_yandex_autocompletion\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Disabling PyTorch because PyTorch >= 2.1 is required but found 2.0.1+cu118\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 valid:  68%|██████▊   | 823/1205 [00:51<00:21, 17.39batch/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from src.dataset import NextTokenDataset\n",
    "from src.model import RNNAutocompletion\n",
    "\n",
    "\n",
    "EXP_NAME = \"exp1\"\n",
    "TRAIN_MAX_LENGTH = 32\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 2048\n",
    "LR = 2e-3\n",
    "DIM = 256\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\", use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "dataset_train = NextTokenDataset(data_train, pad_token=pad_token_id, max_length=TRAIN_MAX_LENGTH)\n",
    "dataset_val = NextTokenDataset(data_val, pad_token=pad_token_id, max_length=TRAIN_MAX_LENGTH)\n",
    "\n",
    "dl_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "dl_val = DataLoader(dataset_val, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "model = RNNAutocompletion(\n",
    "    vocab_size=vocab_size,\n",
    "    pad_token_id=pad_token_id,\n",
    "    dim=DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT\n",
    "    ).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "save_path = os.path.join(\"exp\", EXP_NAME)\n",
    "os.makedirs(f\"{save_path}/weights\", exist_ok=True)\n",
    "writer = SummaryWriter(log_dir=f\"{save_path}/logs\")\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "train_step = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dl_val, desc=f\"Epoch {epoch} valid\", unit=\"batch\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            lengths = batch[\"length\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            logits = model(input_ids, lengths)\n",
    "            loss = criterion(logits, labels)\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.numel()\n",
    "\n",
    "    val_loss = np.mean(val_losses)\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    print(f\"epoch {epoch} valid loss: {val_loss:.4f} | acc: {val_acc:.4f}\")\n",
    "    writer.add_scalar(\"Loss/valid\", val_loss, epoch)\n",
    "    writer.add_scalar(\"Acc/valid\",  val_acc,  epoch)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), f\"{save_path}/weights/best.pt\")\n",
    "\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "\n",
    "    for batch in tqdm(dl_train, desc=f\"Epoch {epoch} train\", unit=\"batch\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        lengths = batch[\"length\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, lengths)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        writer.add_scalar(\"Loss/train_step\", loss.item(), train_step)\n",
    "        train_step += 1\n",
    "\n",
    "    train_loss = np.mean(train_losses)\n",
    "    print(f\"epoch {epoch} train loss: {train_loss:.4f}\")\n",
    "    writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), f\"{save_path}/weights/last.pt\")\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
