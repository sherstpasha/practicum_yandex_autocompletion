{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c50a7393",
   "metadata": {},
   "source": [
    "# 1 Предобработка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c1284c",
   "metadata": {},
   "source": [
    "### 1.1 Очистка и нормализация текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14056af5",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\n",
    "    r'data\\raw_dataset.csv',\n",
    "    encoding='latin-1',\n",
    "    header=None,\n",
    "    names=['target', 'id', 'date', 'flag', 'user', 'text']\n",
    ")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a48509",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3fddc1",
   "metadata": {},
   "source": [
    "В нижний регистр"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250fb531",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lower = data['text'].str.lower()\n",
    "\n",
    "text_lower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131d80ea",
   "metadata": {},
   "source": [
    "Удаление ссылок итд + удаление дубликатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6f641e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# ссылки и домены\n",
    "URL_RE = re.compile(r'(https?://\\S+|www\\.\\S+|[a-zA-Z0-9\\-]+\\.[a-zA-Z]{2,})')\n",
    "# упоминания\n",
    "MENTION_RE = re.compile(r'@\\w+')\n",
    "# хэштеги\n",
    "HASHTAG_RE = re.compile(r'#\\w+')\n",
    "\n",
    "# оставить только латиницу/цифры/пробелы и пунктуацию . , '\n",
    "ALLOWED_CHARS_RE = re.compile(r\"[^a-z0-9\\s\\.\\,']\")\n",
    "\n",
    "# схлопывание повторов:\n",
    "# 1) буквы: 3+ одинаковых подряд -> 2 (cooool -> coool -> coo)\n",
    "LETTER_RUNS_RE = re.compile(r'([a-z])\\1{2,}')\n",
    "# 2) пунктуация . , ' : 2+ -> 1 (.... -> .  ,, -> ,  ''' -> ')\n",
    "PUNCT_RUNS_RE = re.compile(r\"([\\.\\,'])\\1+\")\n",
    "\n",
    "def clean(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "\n",
    "    # убрать ссылки/упоминания/хэштеги\n",
    "    text = URL_RE.sub(\" \", text)\n",
    "    text = MENTION_RE.sub(\" \", text)\n",
    "    text = HASHTAG_RE.sub(\" \", text)\n",
    "\n",
    "    # убрать все, кроме [a-z0-9] пробелов и . , '\n",
    "    text = ALLOWED_CHARS_RE.sub(\" \", text)\n",
    "\n",
    "    # схлопнуть длинные буквы (оставить максимум две подряд)\n",
    "    text = LETTER_RUNS_RE.sub(r\"\\1\\1\", text)\n",
    "\n",
    "    # схлопнуть повторы пунктуации до одной\n",
    "    text = PUNCT_RUNS_RE.sub(r\"\\1\", text)\n",
    "\n",
    "    # убрать лишние пробелы\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "clean_text_lower = text_lower.apply(clean)\n",
    "clean_text_lower = clean_text_lower.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "clean_text_lower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b081e2",
   "metadata": {},
   "source": [
    "Статистика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a29053",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(w) for s in clean_text_lower for w in s.split()]\n",
    "\n",
    "\n",
    "print(pd.Series(lengths, name=\"word_len\").describe().round(2))\n",
    "pd.Series(lengths, name=\"word_len\").hist(bins = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66551f6",
   "metadata": {},
   "source": [
    "### 1.2 Токенизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d695e79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\", use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "texts = clean_text_lower.astype(str).tolist()\n",
    "\n",
    "enc = tokenizer(\n",
    "    texts,\n",
    "    truncation=True,\n",
    "    padding=False,\n",
    "    max_length=32,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors=None\n",
    ")\n",
    "\n",
    "input_ids = [ids + [tokenizer.eos_token_id] for ids in enc[\"input_ids\"]]\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44539494",
   "metadata": {},
   "source": [
    "### 1.3 Разделение на трейн, валидацию и тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d8a8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "data = pd.DataFrame({'text': texts, \"input_ids\": input_ids})\n",
    "\n",
    "train_df, temp_df = train_test_split(data, test_size=0.2, random_state=42, shuffle=True)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1c725a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "train_df.to_csv('data/train.csv', index = False)\n",
    "val_df.to_csv('data/val.csv', index = False)\n",
    "test_df.to_csv('data/test.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22caa2ad",
   "metadata": {},
   "source": [
    "## 2 Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f876c2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from src.dataset import NextTokenDataset\n",
    "from src.model import RNNAutocompletion\n",
    "\n",
    "\n",
    "data_train = pd.read_csv(r\"data\\train.csv\")\n",
    "data_train[\"input_ids\"] = data_train[\"input_ids\"].apply(ast.literal_eval)\n",
    "\n",
    "data_val = pd.read_csv(r\"data\\val.csv\")\n",
    "data_val[\"input_ids\"] = data_val[\"input_ids\"].apply(ast.literal_eval)\n",
    "\n",
    "\n",
    "EXP_NAME = \"exp2\"\n",
    "TRAIN_MAX_LENGTH = 32\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 4048\n",
    "LR = 2e-3\n",
    "DIM = 512\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\", use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "dataset_train = NextTokenDataset(data_train, pad_token=pad_token_id, max_length=TRAIN_MAX_LENGTH)\n",
    "dataset_val = NextTokenDataset(data_val, pad_token=pad_token_id, max_length=TRAIN_MAX_LENGTH)\n",
    "\n",
    "dl_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "dl_val = DataLoader(dataset_val, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "model = RNNAutocompletion(\n",
    "    vocab_size=vocab_size,\n",
    "    pad_token_id=pad_token_id,\n",
    "    dim=DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT\n",
    "    ).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=10, T_mult=2, eta_min=1e-5\n",
    ")\n",
    "\n",
    "save_path = os.path.join(\"exp\", EXP_NAME)\n",
    "os.makedirs(f\"{save_path}/weights\", exist_ok=True)\n",
    "writer = SummaryWriter(log_dir=f\"{save_path}/logs\")\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "train_step = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    val_correct = 0\n",
    "    val_correct_top5 = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        for batch in tqdm(dl_val, desc=f\"Epoch {epoch} valid\", unit=\"batch\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            lengths = batch[\"length\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            logits = model(input_ids, lengths)\n",
    "            loss = criterion(logits, labels)\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.numel()\n",
    "\n",
    "            _, top5 = logits.topk(5, dim=-1)\n",
    "            val_correct_top5 += (top5 == labels.unsqueeze(-1)).any(dim=-1).sum().item()\n",
    "\n",
    "    val_loss = np.mean(val_losses)\n",
    "    val_ppl = np.exp(val_loss)\n",
    "    val_acc = val_correct / val_total\n",
    "    val_acc_top5 = val_correct_top5 / val_total\n",
    "\n",
    "    print(f\"epoch {epoch} valid loss: {val_loss:.4f} | ppl: {val_ppl:.2f} \"f\"| acc@1: {val_acc:.4f} | acc@5: {val_acc_top5:.4f}\")\n",
    "    writer.add_scalar(\"Loss/valid\", val_loss, epoch)\n",
    "    writer.add_scalar(\"Perplexity/valid\", val_ppl, epoch)\n",
    "    writer.add_scalar(\"Acc/valid\",  val_acc,  epoch)\n",
    "    writer.add_scalar(\"Acc/valid_top5\", val_acc_top5, epoch)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), f\"{save_path}/weights/best.pt\")\n",
    "\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "\n",
    "    for batch in tqdm(dl_train, desc=f\"Epoch {epoch} train\", unit=\"batch\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        lengths = batch[\"length\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            logits = model(input_ids, lengths)\n",
    "            loss = criterion(logits, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)                                \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) \n",
    "        scaler.step(optimizer)                                    \n",
    "        scaler.update()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        writer.add_scalar(\"Loss/train_step\", loss.item(), train_step)\n",
    "        scheduler.step()\n",
    "        train_step += 1\n",
    "\n",
    "    train_loss = np.mean(train_losses)\n",
    "    print(f\"epoch {epoch} train loss: {train_loss:.4f}\")\n",
    "    writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "    writer.add_scalar(\"LR\", scheduler.get_last_lr()[0], train_step)\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), f\"{save_path}/weights/last.pt\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf3db42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import ast\n",
    "from transformers import AutoTokenizer\n",
    "import evaluate\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.model import RNNAutocompletion\n",
    "from src.dataset import NextTokenDataset\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "DIM = 256\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.3\n",
    "MODEL_PATH = \"exp/exp1/weights/best.pt\"\n",
    "TRAIN_MAX_LENGTH = 32\n",
    "BATCH_SIZE = 1024\n",
    "TEST_DATA_PATH = r\"data\\test.csv\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\", use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "data_test = pd.read_csv(TEST_DATA_PATH)\n",
    "data_test[\"input_ids\"] = data_test[\"input_ids\"].apply(ast.literal_eval)\n",
    "\n",
    "dataset_test = NextTokenDataset(data_test, pad_token=pad_token_id, max_length=TRAIN_MAX_LENGTH)\n",
    "dl_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "model = RNNAutocompletion(\n",
    "    vocab_size=vocab_size,\n",
    "    pad_token_id=pad_token_id,\n",
    "    dim=DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "checkpoint = torch.load(MODEL_PATH, map_location=device)\n",
    "model.load_state_dict(checkpoint)\n",
    "model.eval()\n",
    "\n",
    "test_correct = 0\n",
    "test_correct_top5 = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    for batch in tqdm(dl_test, desc=f\"Test\", unit=\"batch\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        lengths = batch[\"length\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        logits = model(input_ids, lengths)\n",
    "\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        test_correct += (preds == labels).sum().item()\n",
    "        test_total += labels.numel()\n",
    "\n",
    "        _, top5 = logits.topk(5, dim=-1)\n",
    "        test_correct_top5 += (top5 == labels.unsqueeze(-1)).any(dim=-1).sum().item()\n",
    "\n",
    "test_acc = test_correct / test_total\n",
    "test_acc_top5 = test_correct_top5 / test_total\n",
    "\n",
    "print(f\"Test acc@1: {test_acc:.4f} | acc@5: {test_acc_top5:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7307d4",
   "metadata": {},
   "source": [
    "## Подбор параметров генерации для distilgpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c8119d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7963aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen: 100%|██████████| 299/299 [09:04<00:00,  1.82s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': np.float64(0.0), 'rouge2': np.float64(0.0), 'rougeL': np.float64(0.0), 'rougeLsum': np.float64(0.0)}\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import pandas as pd\n",
    "import ast\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# === Константы ===\n",
    "VAL_DATA_PATH     = r\"data\\val.csv\"\n",
    "GPT_MODEL_NAME    = \"distilgpt2\"\n",
    "BATCH_SIZE        = 1024\n",
    "TRAIN_MAX_LENGTH  = 32    # сколько токенов берём в контекст\n",
    "MAX_REF_TOKENS    = 32    # сколько токенов берём в референс (и генерим)\n",
    "\n",
    "# === Данные ===\n",
    "data_val = pd.read_csv(VAL_DATA_PATH)\n",
    "data_val[\"input_ids\"] = data_val[\"input_ids\"].apply(ast.literal_eval)\n",
    "\n",
    "# === Токенизатор/модель/пайплайн ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(GPT_MODEL_NAME, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(GPT_MODEL_NAME).to(device)\n",
    "if device.type == \"cuda\":\n",
    "    model = model.half()\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if device.type == \"cuda\" else -1\n",
    ")\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def ids_to_text(ids):\n",
    "    return tokenizer.decode(ids, skip_special_tokens=True)\n",
    "\n",
    "# === Готовим пары (context_text, reference_text) прямо из input_ids ===\n",
    "contexts, references = [], []\n",
    "for ids in data_val[\"input_ids\"]:\n",
    "    n = len(ids)\n",
    "    if n < 2:\n",
    "        continue\n",
    "    pivot = min(TRAIN_MAX_LENGTH, n-1)           # гарантируем, что GT не пустой\n",
    "    ctx_ids = ids[:pivot]\n",
    "    ref_ids = ids[pivot : min(n, pivot + MAX_REF_TOKENS)]\n",
    "\n",
    "    contexts.append(ids_to_text(ctx_ids))\n",
    "    references.append(ids_to_text(ref_ids))\n",
    "\n",
    "print(len(contexts), len(data_val[\"input_ids\"]))\n",
    "\n",
    "\n",
    "# === Генерация батчами (только продолжение, без контекста) ===\n",
    "predictions = []\n",
    "for i in tqdm(range(0, len(contexts), BATCH_SIZE), desc=\"Gen\", unit=\"batch\"):\n",
    "    batch_ctx = contexts[i:i+BATCH_SIZE]\n",
    "    outs = generator(\n",
    "        batch_ctx,\n",
    "        max_new_tokens=MAX_REF_TOKENS,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        temperature=0.8,\n",
    "        return_full_text=False,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "    preds_batch = [o[0][\"generated_text\"] for o in outs]\n",
    "    predictions.extend(preds_batch)\n",
    "\n",
    "# === ROUGE по “чистым” продолжениям\n",
    "scores = rouge.compute(predictions=predictions, references=references)\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b57dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "batch_size = 16\n",
    "context_lengths = [1, 3, 9]\n",
    "max_gen_tokens = 32\n",
    "\n",
    "for n in context_lengths:\n",
    "    print(f\"\\n==== Context length: {n} ====\")\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "\n",
    "    for i in range(0, len(data_test.input_ids), batch_size):\n",
    "        batch_messages = data_test.input_ids[i : i + batch_size]\n",
    "\n",
    "        contexts = [msg[:n] for msg in batch_messages]\n",
    "        contexts = torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.tensor(c, dtype=torch.long) for c in contexts],\n",
    "            batch_first=True,\n",
    "            padding_value=pad_token_id\n",
    "        ).to(device)\n",
    "\n",
    "        references = [msg[n:] for msg in batch_messages]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_batch = model.generate(start_tokens=contexts, max_tokens=max_gen_tokens)\n",
    "\n",
    "        for gen_tokens, ref in zip(generated_batch, references):\n",
    "            pred_text = tokenizer.decode(gen_tokens.tolist(), skip_special_tokens=True)\n",
    "            ref_text = tokenizer.decode(ref, skip_special_tokens=True)\n",
    "\n",
    "            all_predictions.append(pred_text)\n",
    "            all_references.append(ref_text)\n",
    "\n",
    "    results = rouge.compute(predictions=all_predictions, references=all_references)\n",
    "    print(f\"ROUGE scores for context={n}: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e234f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
